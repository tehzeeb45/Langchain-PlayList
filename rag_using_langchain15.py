# -*- coding: utf-8 -*-
"""rag-using-Langchain15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t_nhy0lGoHRWuOWHUWNV3rdb639eAZ-Z
"""

!pip install -q youtube-transcript-api langchain-community langchain-openai \
               faiss-cpu tiktoken python-dotenv

!pip install youtube-transcript-api --upgrade


from youtube_transcript_api import YouTubeTranscriptApi,  TranscriptsDisabled

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
!pip install langchain-huggingface
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline

"""Step1a: Indexing(Document Ingestion)

"""

from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled

video_id = "8jazNUpO3lQ"

try:
    yt = YouTubeTranscriptApi()
    transcript_list = yt.fetch(video_id, languages=["en"])
    transcript = " ".join(chunk.text for chunk in transcript_list)  # .text attribute use karo
    print(transcript)
except TranscriptsDisabled:
    print("No captions available for this video.")

"""Step1b: Indexing(Text Splitting)"""

splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)
chunks = splitter.create_documents([transcript])

len(chunks)

chunks[0]

"""Step1c: Embedding and Store in Vectore Store


"""

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectore_Store = FAISS.from_documents(chunks, embedding_model)

vectore_Store.index_to_docstore_id

"""Step 2: Retriever"""

retriever = vectore_Store.as_retriever(search_type="similarity", search_kwargs={"k" : 1})

retriever.invoke("Explain the example of predict home prices using linear regression")

"""Step 3: Augmentation"""

from transformers import pipeline
generator = pipeline(
    "text2text-generation",
    model="google/flan-t5-xl",   # ya koi bhi HF model
    max_new_tokens=300,
    temperature=0.3
)

# Wrap it in LangChain LLM
llm = HuggingFacePipeline(pipeline=generator)

prompt = PromptTemplate(
    template="""
  You are a helpful assistant.
Answer ONLY from the provided transcript context.
Explain your answer in detail, step by step.
Do not answer just yes/no.
.

      {context}
      Question: {question}
    """,
    input_variables = ['context', 'question']
)

question = "Explain the example of predict home prices using linear regression? Explain step by step"
retrieved_doc = retriever.invoke(question)

retrieved_doc

context_text = "\n\n".join(doc.page_content for doc in retrieved_doc)

final_prompt = prompt.invoke({"context" : context_text, "question" : question})

final_prompt

"""Step 4: Generation"""

answer = llm.invoke(final_prompt.to_string())
print(answer)

"""Building a Chain"""

from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

def format_docs(retrieved_docs):
  context_text = "\n\n".join(doc.page_content for doc in retrieved_doc)
  return context_text

parallel_chain = RunnableParallel({
    'context' : retriever | RunnableLambda(format_docs),
    'question' : RunnablePassthrough()
})

parser = StrOutputParser

main_chain = parallel_chain | prompt | llm | parser

main_chain.invoke('Explain the example of predict home prices using linear regression? Explain step by step')

