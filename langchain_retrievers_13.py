# -*- coding: utf-8 -*-
"""Langchain-retrievers_13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ymFYkLNVa63-bTJn_47TTIgDOh4Rz1Re
"""

!pip install langchain chromadb faiss-cpu openai tiktoken langchain_openai langchain-community wikipedia

## 1. Wikipedia Retriever
from langchain_community.retrievers import WikipediaRetriever

# Create Retriver object
retriever = WikipediaRetriever(top_k_results=2, lang="en")

# Define your query
query = "the geopolitical history of india and pakistan from the perspective of a chinese"
docs = retriever.invoke(query)

docs

# Print retrievers content
for i, doc in enumerate(docs):
  print(f"\n--- Result {i+1} ---")
  print(f"Content:\n{doc.page_content}...")

# 2. Vector Store Retrievers
!pip install langchain-community
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

# Step 1: Your source documents
documents = [
    Document(page_content="LangChain helps developers build LLM applications easily."),
    Document(page_content="Chroma is a vector database optimized for LLM-based search."),
    Document(page_content="Embeddings convert text into high-dimensional vectors."),
    Document(page_content="OpenAI provides powerful embedding models."),
]

# Load Hugging Face embedding model (free)
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create Chroma vector store
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embedding_model,
    collection_name="my_collection"
)

# Convert vectorestore into a retriever
retriever = vectorstore.as_retriever(search_kwargs={"k":2})

query = "What is Chroma used for?"
results = retriever.invoke(query)

results

for i, doc in enumerate(results):
  print(f"\n--- Results {i+1} ---")
  print(doc.page_content)

# 3. MMR
docs = [
    Document(page_content="LangChain makes it easy to work with LLMs."),
    Document(page_content="LangChain is used to build LLM based applications."),
    Document(page_content="Chroma is used to store and search document embeddings."),
    Document(page_content="Embeddings are vector representations of text."),
    Document(page_content="MMR helps you get diverse results when doing similarity search."),
    Document(page_content="LangChain supports Chroma, FAISS, Pinecone, and more."),
]

!pip install faiss-cpu
from langchain_community.vectorstores import FAISS
# Load Hugging Face embedding model (free)
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# Create a vector Store
vectorstore = FAISS.from_documents(
    documents = docs,
    embedding=embedding_model
)

# Enable MMR in the retriever
retriever = vectorstore.as_retriever(
    search_type = "mmr",
    search_kwargs={"k": 3, "lambda_mult":0.5}
)

query = "What is langchain"
results = retriever.invoke(query)

for i, doc in enumerate(results):
    print(f"\n--- Result {i+1} ---")
    print(doc.page_content)

# 4. Mulit query Rerrievers
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_community.llms import HuggingFaceHub
from langchain.retrievers.multi_query import MultiQueryRetriever

# Relevant health and wellness documents
# Relevant health & wellness documents
all_docs = [
    Document(page_content="Regular walking boosts heart health and can reduce symptoms of depression.", metadata={"source": "H1"}),
    Document(page_content="Consuming leafy greens and fruits helps detox the body and improve longevity.", metadata={"source": "H2"}),
    Document(page_content="Deep sleep is crucial for cellular repair and emotional regulation.", metadata={"source": "H3"}),
    Document(page_content="Mindfulness and controlled breathing lower cortisol and improve mental clarity.", metadata={"source": "H4"}),
    Document(page_content="Drinking sufficient water throughout the day helps maintain metabolism and energy.", metadata={"source": "H5"}),
    Document(page_content="The solar energy system in modern homes helps balance electricity demand.", metadata={"source": "I1"}),
    Document(page_content="Python balances readability with power, making it a popular system design language.", metadata={"source": "I2"}),
    Document(page_content="Photosynthesis enables plants to produce energy by converting sunlight.", metadata={"source": "I3"}),
    Document(page_content="The 2022 FIFA World Cup was held in Qatar and drew global energy and excitement.", metadata={"source": "I4"}),
    Document(page_content="Black holes bend spacetime and store immense gravitational energy.", metadata={"source": "I5"}),
]

# Load Hugging Face embedding model (free)
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

vectorstore = FAISS.from_documents(documents=all_docs,embedding=embedding_model)

# Create retrievers
similarity_retriever = vectorstore.as_retriever(search_type = "similarity", search_kwargs ={"k":5})

# Hugging Face token
import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "*****************************  "

# Load HF model (example: FLAN-T5 for text generation)
llm = HuggingFaceHub(
    repo_id="google/flan-t5-base",
    task="text2text-generation",
    model_kwargs={"temperature": 0.3, "max_length": 256}
)

# MultiQueryRetriever with Hugging Face LLM
multiquery_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    llm=llm
)

query = "How to imrove energy level and maintain balance"

similarity_results = similarity_retriever.invoke(query)
#multiquery_results = multiquery_retriever.invoke(query)

from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline

# Local pipeline (no Hugging Face Hub token needed)
generator = pipeline(
    "text2text-generation",
    model="google/flan-t5-base"
)

llm = HuggingFacePipeline(pipeline=generator)

from langchain.retrievers.multi_query import MultiQueryRetriever

multiquery_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    llm=llm
)

query = "Who is the captain of CSK?"
multiquery_results = multiquery_retriever.invoke(query)

for i, doc in enumerate(multiquery_results):
    print(f"\n--- Result {i+1} ---")
    print(doc.page_content)

for i, doc in enumerate(similarity_results):
    print(f"\n--- Result {i+1} ---")
    print(doc.page_content)

#  5. Contexual Compression Retriever
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_community.llms import HuggingFaceHub
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.contextual_compression import ContextualCompressionRetriever

# Recreate the document objects from the previous data
docs = [
    Document(page_content=(
        """The Grand Canyon is one of the most visited natural wonders in the world.
        Photosynthesis is the process by which green plants convert sunlight into energy.
        Millions of tourists travel to see it every year. The rocks date back millions of years."""
    ), metadata={"source": "Doc1"}),

    Document(page_content=(
        """In medieval Europe, castles were built primarily for defense.
        The chlorophyll in plant cells captures sunlight during photosynthesis.
        Knights wore armor made of metal. Siege weapons were often used to breach castle walls."""
    ), metadata={"source": "Doc2"}),

    Document(page_content=(
        """Basketball was invented by Dr. James Naismith in the late 19th century.
        It was originally played with a soccer ball and peach baskets. NBA is now a global league."""
    ), metadata={"source": "Doc3"}),

    Document(page_content=(
        """The history of cinema began in the late 1800s. Silent films were the earliest form.
        Thomas Edison was among the pioneers. Photosynthesis does not occur in animal cells.
        Modern filmmaking involves complex CGI and sound design."""
    ), metadata={"source": "Doc4"})
]

# Load Hugging Face embedding model (free)
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(documents=all_docs,embedding=embedding_model)

base_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

from huggingface_hub import InferenceClient
from langchain_community.llms import HuggingFaceHub
from langchain.retrievers.document_compressors import LLMChainExtractor
import os

# 1. Local Hugging Face pipeline (FLAN-T5)
generator = pipeline(
    "text2text-generation",
    model="google/flan-t5-base"
)

# 2. Wrap into LangChain LLM
llm = HuggingFacePipeline(pipeline=generator)
# 5. Compressor setup
compressor = LLMChainExtractor.from_llm(llm)

# Create the contextual compression retriever
compression_retriever = ContextualCompressionRetriever(
    base_retriever=base_retriever,
    base_compressor=compressor
)

# Query the retriever
query = "What is photosynthesis?"
compressed_results = compression_retriever.invoke(query)

# 8. Show results
for i, doc in enumerate(compressed_results):
    print(f"\n--- Result {i+1} ---")
    print(doc.page_content)

